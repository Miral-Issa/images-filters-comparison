{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and preparation:\n",
    "\n",
    "I collected 15 images with different levels of details. Some had many details, others had overlapping objects, or obvious edges. Others had less details, or less details in some parts of the image, or almost no details. Some had many colours, others had only two.\n",
    "\n",
    "I tried as much as possible to collect different images that would highlight different effects of different problems, or filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "image_folder = '/content/original_images'\n",
    "\n",
    "data = []\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img = cv2.imread(os.path.join(image_folder, filename))\n",
    "        if img is not None:\n",
    "            # Convert to grayscale\n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            data.append({'filename': filename, 'image': gray_img})\n",
    "\n",
    "original = pd.DataFrame(data)\n",
    "print(original.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, row in original.iterrows():\n",
    "    img_rgb = cv2.cvtColor(row['image'], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the original images' dataframe I need to create noisy images.\n",
    "I want to create different dataframes for different types of errors and noise, so when I apply the filters later I can clearly see each filter's effect on different types of noise.\n",
    "\n",
    "The first dataframe is 'salty', one that contains the same images with salt and pepper noise added to them at different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_salt_pepper_noise(image, prob):\n",
    "    noisy_image = image.copy()\n",
    "\n",
    "    num_salt = np.ceil(prob * image.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1]] = 1\n",
    "\n",
    "    num_pepper = np.ceil(prob * image.size * 0.5)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape]\n",
    "    noisy_image[coords[0], coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "noise_levels = [0.01, 0.08, 0.2]\n",
    "\n",
    "noisy_data = []\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    for level in noise_levels:\n",
    "        noisy_img = add_salt_pepper_noise(row['image'], level)\n",
    "        noisy_data.append({\n",
    "            'filename': f\"{row['filename']}_noise_{level}\",\n",
    "            'image': noisy_img,\n",
    "            'noise_level': level\n",
    "        })\n",
    "\n",
    "\n",
    "salty = pd.DataFrame(noisy_data)\n",
    "print(salty.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, row in salty.iterrows():\n",
    "    img_rgb = cv2.cvtColor(row['image'], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now the salty dataframe contains 3 copies of each original image with 3 different levels of added salt and pepper noise.\n",
    "\n",
    "Just as before a new dataframe called gaussian_noise is created, but this time a Gaussian noise was added to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_gaussian_noise(image, var, mean=20):\n",
    "    sigma = var**0.5\n",
    "    gaussian_noise = np.random.normal(mean, sigma, image.shape)\n",
    "\n",
    "    noisy_image = image + gaussian_noise\n",
    "\n",
    "    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "gaussian_variances = [500, 1000, 2000]\n",
    "\n",
    "noisy_data_gaussian = []\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    for var in gaussian_variances:\n",
    "        noisy_img = add_gaussian_noise(row['image'], var=var)\n",
    "        noisy_data_gaussian.append({\n",
    "            'filename': f\"{row['filename']}_gaussian_{var}\",\n",
    "            'image': noisy_img,\n",
    "            'variance': var\n",
    "        })\n",
    "\n",
    "gaussian_noise = pd.DataFrame(noisy_data_gaussian)\n",
    "print(gaussian_noise.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_cols = 3\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(original), ncols=num_cols, figsize=(20, len(original) * 5))\n",
    "\n",
    "for i, (index, row) in enumerate(original.iterrows()):\n",
    "    for j, var in enumerate(gaussian_variances):\n",
    "        noisy_img = gaussian_noise[\n",
    "            (gaussian_noise['filename'] == f\"{row['filename']}_gaussian_{var}\")\n",
    "        ]['image'].values[0]\n",
    "\n",
    "        img_rgb = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        axes[i, j].imshow(img_rgb)\n",
    "        axes[i, j].set_title(f\"{row['filename']} - Var: {var}\")\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other types of noise that images could suffer from is the Poisson noise or shot noise, which often appears in images captured by cameras, especially in low-light conditions. image can end up looking grainy and unclean because of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def add_poisson_noise(image):\n",
    "    noisy_image = np.random.poisson(image).astype(np.uint8)\n",
    "    noisy_image = np.clip(noisy_image, 0, 255)\n",
    "    return noisy_image\n",
    "\n",
    "noisy_data_poisson = []\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    noisy_img = add_poisson_noise(row['image'])\n",
    "    noisy_data_poisson.append({\n",
    "        'filename': f\"{row['filename']}_poisson\",\n",
    "        'image': noisy_img,\n",
    "        'noise_type': 'poisson'\n",
    "    })\n",
    "\n",
    "noisy_poisson = pd.DataFrame(noisy_data_poisson)\n",
    "print(noisy_poisson.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, row in noisy_poisson.iterrows():\n",
    "    img_rgb = cv2.cvtColor(row['image'], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speckl noise:\n",
    "\n",
    "\"Speckle noise is a form of noise that commonly occurs in images acquired through coherent imaging techniques such as ultrasound imaging, synthetic aperture radar (SAR), and laser imaging. Unlike Gaussian noise or salt and pepper noise, which manifest as additive disturbances, speckle noise is multiplicative in nature. This means that it affects the variance of pixel values rather than their mean intensity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_speckle_noise(image_shape, mean=0, variance=0.01):\n",
    "    noise = np.random.normal(mean, variance**0.5, size=image_shape)\n",
    "    speckle_noise = noise * np.ones(image_shape)\n",
    "    return speckle_noise\n",
    "\n",
    "def add_speckle_noise(image, mean=0, variance=0.1):\n",
    "    speckle_noise = generate_speckle_noise(image.shape, mean=mean, variance=variance)\n",
    "    noisy_image = image + image * speckle_noise\n",
    "    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "    return noisy_image\n",
    "\n",
    "speckle_variances = [0.05, 0.1, 0.2]\n",
    "\n",
    "noisy_data_speckle = []\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    for var in speckle_variances:\n",
    "        noisy_img = add_speckle_noise(row['image'], variance=var)\n",
    "        noisy_data_speckle.append({\n",
    "            'filename': f\"{row['filename']}_speckle_{var}\",\n",
    "            'image': noisy_img,\n",
    "            'variance': var\n",
    "        })\n",
    "\n",
    "speckle_noise = pd.DataFrame(noisy_data_speckle)\n",
    "print(speckle_noise.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "for index, row in speckle_noise.iterrows():\n",
    "    img_rgb = cv2.cvtColor(row['image'], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the many errors and noises that we can find in images in real life, and while some of them may need contrast enhancement, or other image enhancement techniques before using filters, I'm coruse to find if the different types of filters could fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Filters:\n",
    "now let us apply different types of filters on these noisy images we generated, and notice how much they're affective in solving their problems, compared to the original images, the ones with no noise.\n",
    "\n",
    "first let us study simple filter:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Computational_times = {}\n",
    "avarage_MSE = {}\n",
    "avarage_psnr = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.   Box filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salt and Pepper noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Computational_times[\"Salt and Pepper\"] = []\n",
    "avarage_MSE[\"Salt and Pepper\"] = []\n",
    "avarage_psnr[\"Salt and Pepper\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_box_filter(image, ksize=(5, 5)):\n",
    "    filtered_image = cv2.boxFilter(image, ddepth=-1, ksize=ksize)\n",
    "    return filtered_image\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in salty.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(5, 5))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Salt and Pepper\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)\n",
    "print(filtered_images.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mse(original, filtered):\n",
    "    return np.mean((original - filtered) ** 2)\n",
    "\n",
    "def calculate_psnr(mse, max_pixel=255.0):\n",
    "    return 20 * np.log10(max_pixel) - 10 * np.log10(mse) if mse > 0 else float('inf')\n",
    "\n",
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Salt and Pepper\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Salt and Pepper\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the previous output the box filter with kernal size of 5 by 5 didn't perform well on the images, and it did have a somewhat high values of MSE and PSNR, especially on images with median and high level of noise.\n",
    "\n",
    "However, it indeed completed the work in a short time of only 1.32 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_canny_edge(image, threshold1=100, threshold2=200):\n",
    "    edges = cv2.Canny(image, threshold1, threshold2)\n",
    "    return edges\n",
    "\n",
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canny edge detection helped visualize the box filter short coming, as it detected much noise as edges in 2 thirds of the images (images with median and high levels of noise).\n",
    "\n",
    "These output are almost the same in all images regardless of the amount of details in them. while of course the ones with higher level of details suffering the most. while being more visible with images with a wide area of low information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try the same but this time with a bigger kernel, let's say an 11 by 11 one. over the douple of the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_box_filter(image, ksize=(5, 5)):\n",
    "    filtered_image = cv2.boxFilter(image, ddepth=-1, ksize=ksize)\n",
    "    return filtered_image\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in salty.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(11, 11))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Salt and Pepper\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_mse(original, filtered):\n",
    "    return np.mean((original - filtered) ** 2)\n",
    "\n",
    "def calculate_psnr(mse, max_pixel=255.0):\n",
    "    return 20 * np.log10(max_pixel) - 10 * np.log10(mse) if mse > 0 else float('inf')\n",
    "\n",
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Salt and Pepper\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Salt and Pepper\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from these measures, and from the visualizations, the big kernel didn't perform bitter. as the nature of the salt and pepper noise could not be solved by reducing the gaps between neighboring pixels.\n",
    "\n",
    "And with a bigger kernel the box filter method would calculate a not neccesary close values to each other, due to the nature of this noise.\n",
    "\n",
    "As in a big kernel many pepper noise could exist with salt noise, they could cancel each other, or worsen the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply_canny_edge(image, threshold1=100, threshold2=200):\n",
    "    edges = cv2.Canny(image, threshold1, threshold2)\n",
    "    return edges\n",
    "\n",
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the bigger kernel blurred the thin and weak edges with the background. Hence why Canny no longer could detect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Computational_times[\"Gaussian\"] = []\n",
    "avarage_MSE[\"Gaussian\"] = []\n",
    "avarage_psnr[\"Gaussian\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in gaussian_noise.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(5, 5))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Gaussian\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Gaussian\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Gaussian\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images this time look smoother and more visually appealing, as the box filter works well with Gaussian noise. It works on bringing the pixels values closer to each other (to their neighbors), and Gaussian noise tend to be gradual. However, it doesn't work on returning the pixel to its original value, hence why the average MSE and PSNR are still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As could be seen, much of the fine details got too blurred to be detected by Canny as edges. As the Gaussian noise mush them, and the box filter blurs them even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in gaussian_noise.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(11, 11))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Gaussian\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Gaussian\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Gaussian\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as with salt and pepper noise, with a bigger kernel the images got smoother, yet the details got more blurred, leading to edges detection suffering even more. Now only very strong and broad edges are detected. The image skeleton is unobserved in most images, especially images with many details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Computational_times[\"Poisson\"] = []\n",
    "avarage_MSE[\"Poisson\"] = []\n",
    "avarage_psnr[\"Poisson\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in noisy_poisson.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(5, 5))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Poisson\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Poisson\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Poisson\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the filter smoothed the image, still some of the darkened parts of the images because of the noise remained visible. And even pulled the filter to darken the image more in low-detailed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low-detailed images suffered from noise being detected as edges in them. While the high-detailed image suffered from its fine details being too blurred to be detected.\n",
    "\n",
    "That is due to the box calculating higher new values for the pixels affected and being surrounded by noise in low information areas. And blurring the edges with noise in high information areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in noisy_poisson.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(11, 11))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Poisson\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Poisson\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Poisson\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bigger kernel worsened the edges problem, with now even low detailed images with strong and broad edges got too blurred to be detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speckle noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Computational_times[\"Speckle\"] = []\n",
    "avarage_MSE[\"Speckle\"] = []\n",
    "avarage_psnr[\"Speckle\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in speckle_noise.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(5, 5))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Speckle\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Speckle\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Speckle\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter smoothed the images with low and median noise levels, while not effecting the edges too much. However, it couldn't smooth the noise in high noise level images enough. That's why Canny detected the noise as edges in these images.\n",
    "\n",
    "That may hint at using a bigger kernel to solve such a problem. Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for index, row in speckle_noise.iterrows():\n",
    "    filtered_img = apply_box_filter(row['image'], ksize=(11, 11))\n",
    "    filtered_data.append({\n",
    "        'filename': f\"{row['filename']}_filtered\",\n",
    "        'image': filtered_img,\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "Computational_times[\"Speckle\"].append(total_time)\n",
    "print(f\"Total computational time for applying Box filter: {total_time:.4f} seconds\")\n",
    "\n",
    "filtered_images = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "mse_sum = 0\n",
    "psnr_sum = 0\n",
    "\n",
    "for index, row in original.iterrows():\n",
    "    original_img = row['image']\n",
    "\n",
    "    filtered_img_rows = filtered_images[filtered_images['filename'].str.contains(row['filename'])]\n",
    "\n",
    "    for _, filtered_row in filtered_img_rows.iterrows():\n",
    "        filtered_img = filtered_row['image']\n",
    "\n",
    "        mse = calculate_mse(original_img, filtered_img)\n",
    "        mse_sum += mse\n",
    "        psnr = calculate_psnr(mse)\n",
    "        psnr_sum += psnr\n",
    "\n",
    "        counter +=1\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        axes[0].imshow(original_img, cmap='gray')\n",
    "        axes[0].set_title(f\"Original: {row['filename']}\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(filtered_img, cmap='gray')\n",
    "        axes[1].set_title(f\"MSE: {mse:.2f}, PSNR: {psnr:.2f} dB\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "avrg_mse = mse_sum/counter\n",
    "avarage_MSE[\"Speckle\"].append(avrg_mse)\n",
    "print(f\"avarage mse = {avrg_mse}\")\n",
    "\n",
    "avrg_psnr = psnr_sum/counter\n",
    "avarage_psnr[\"Speckle\"].append(avrg_psnr)\n",
    "print(f\"avarage psnr = {avrg_psnr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "canny_data = []\n",
    "\n",
    "for index, row in filtered_images.iterrows():\n",
    "    canny_edges = apply_canny_edge(row['image'], threshold1=100, threshold2=200)\n",
    "    canny_data.append({\n",
    "        'filename': f\"{row['filename']}_canny\",\n",
    "        'image': canny_edges\n",
    "    })\n",
    "\n",
    "canny_edges_df = pd.DataFrame(canny_data)\n",
    "\n",
    "for index, row in canny_edges_df.iterrows():\n",
    "    plt.imshow(row['image'], cmap='gray')\n",
    "    plt.title(row['filename'])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a bigger kernel did smooth the images more, this led to more edges being too blurred to be detected.\n",
    "\n",
    "Now Canny couldn't draw the skeleton for all high-detailed images with most of their edges being originally thin and weak. And even low-detailed images with originaly strong and broad edges, were not detected correctly.\n",
    "\n",
    "\n",
    "All while MSE and PSNR are still high, and the pictures aren't that much visually appealing. In my opinion the trade off wasn't worth it. And going to higher kernels isn't an option if edge detection is the next step in the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box filter results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "comp_times_df = pd.DataFrame(Computational_times, index=['5x5 Kernel', '11x11 Kernel']).T\n",
    "mse_df = pd.DataFrame(avarage_MSE, index=['5x5 Kernel', '11x11 Kernel']).T\n",
    "psnr_df = pd.DataFrame(avarage_psnr, index=['5x5 Kernel', '11x11 Kernel']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Computational Times:\")\n",
    "comp_times_df.style.set_caption(\"Computational Times\").set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very clear that the bigger kernel took a longer computational time in all cases. Still, the box filter as a whole took a very short time compared to other filters. That's due to it being the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nAverage MSEs:\")\n",
    "mse_df.style.set_caption(\"Average MSEs\").set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in all cases a bigger kernel worsens the average MSE (mean square error). That is because the box filter calculates the pixel's new value according to its neighbors' values. A bigger kernel has a higher chance of containing more noisy pixels, which will lead the new value to lean further from the one in the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nAverage PSNRs:\")\n",
    "psnr_df.style.set_caption(\"Average PSNRs\").set_table_styles([{'selector': 'th', 'props': [('border', '1px solid black')]}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PSNR (peak signal to noise ratio) stays almost the same in all cases. The box filter smoothes the overall image, bringing the pixels' values closer to their neighbors. Yet as the kernel's weights are distributed equally, this smoothing effect is not that much influential. Even with a bigger kernel."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
